{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc9c7a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/finchmf/coding/control_vectors/experiment_pipeline/env/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/finchmf/coding/control_vectors/experiment_pipeline/env/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/finchmf/coding/control_vectors/experiment_pipeline/env/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/finchmf/coding/control_vectors/experiment_pipeline/env/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 595, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1881, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/finchmf/coding/control_vectors/experiment_pipeline/env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/finchmf/coding/control_vectors/experiment_pipeline/env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/finchmf/coding/control_vectors/experiment_pipeline/env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/finchmf/coding/control_vectors/experiment_pipeline/env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/finchmf/coding/control_vectors/experiment_pipeline/env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/finchmf/coding/control_vectors/experiment_pipeline/env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/finchmf/coding/control_vectors/experiment_pipeline/env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/finchmf/coding/control_vectors/experiment_pipeline/env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/finchmf/coding/control_vectors/experiment_pipeline/env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/finchmf/coding/control_vectors/experiment_pipeline/env/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/finchmf/coding/control_vectors/experiment_pipeline/env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/finchmf/coding/control_vectors/experiment_pipeline/env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/finchmf/coding/control_vectors/experiment_pipeline/env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/cf/s06rmc8s4kq7jrt0jd5q4tx40000gn/T/ipykernel_91729/3616072012.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/Users/finchmf/coding/control_vectors/experiment_pipeline/env/lib/python3.10/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/finchmf/coding/control_vectors/experiment_pipeline/env/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/finchmf/coding/control_vectors/experiment_pipeline/env/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/finchmf/coding/control_vectors/experiment_pipeline/env/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/finchmf/coding/control_vectors/experiment_pipeline/env/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/finchmf/coding/control_vectors/experiment_pipeline/env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/finchmf/coding/control_vectors/experiment_pipeline/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import math\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b14fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(probs: torch.Tensor):\n",
    "    return -torch.sum(probs * torch.log2(probs + 1e-12)).item()\n",
    "\n",
    "def compute_step_metrics(logits: torch.Tensor, k: int = 5):\n",
    "    \"\"\"\n",
    "    From raw logits [vocab_size], compute:\n",
    "      - full entropy\n",
    "      - top-1 probability\n",
    "      - top-k entropy (renormalized over the k largest probs)\n",
    "    \"\"\"\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    entropy = compute_entropy(probs)\n",
    "    top1_prob = torch.max(probs).item()\n",
    "    \n",
    "    topk_vals, _ = torch.topk(probs, k)\n",
    "    topk_probs = topk_vals / topk_vals.sum()\n",
    "    topk_entropy = compute_entropy(topk_probs)\n",
    "    \n",
    "    return {\n",
    "        \"entropy\": entropy,\n",
    "        \"top1_prob\": top1_prob,\n",
    "        f\"top{k}_entropy\": topk_entropy\n",
    "    }\n",
    "\n",
    "def generate_and_analyze(model, tokenizer, prompt: str, max_new_tokens: int = 20, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Manual one‐step‐at‐a‐time generation so we can collect:\n",
    "      - logits → entropy & top-k entropy\n",
    "      - attentions → head-level entropies\n",
    "      - hidden_states → hidden vector norms\n",
    "    Returns generated text + list of per-step metric dicts.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    metrics_per_step = []\n",
    "    generated = input_ids\n",
    "    \n",
    "    for step in range(max_new_tokens):\n",
    "        outputs = model(\n",
    "            generated,\n",
    "            output_attentions=True,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        # take logits for the last token\n",
    "        logits = outputs.logits[:, -1, :]  # [1, vocab_size]\n",
    "        \n",
    "        # 1) basic & top-k entropy\n",
    "        step_m = compute_step_metrics(logits.squeeze(0), k=top_k)\n",
    "        \n",
    "        # 2) head-level attention entropies\n",
    "        # outputs.attentions is a tuple[layer] of shape [1, num_heads, seq_len, seq_len]\n",
    "        head_entropies = []\n",
    "        for layer_att in outputs.attentions:\n",
    "            # attention of last query pos to all keys: shape [num_heads, seq_len]\n",
    "            attn = layer_att[0, :, -1, :]  \n",
    "            # entropy per head\n",
    "            ent = (-attn * torch.log2(attn + 1e-12)).sum(dim=-1)  # [num_heads]\n",
    "            head_entropies.append(ent.tolist())\n",
    "        step_m[\"head_entropies\"] = head_entropies\n",
    "        \n",
    "        # 3) hidden-state norm of new token at final layer\n",
    "        # outputs.hidden_states is tuple[layer] of [1, seq_len, hidden_size]\n",
    "        final_layer_vec = outputs.hidden_states[-1][0, -1, :]  # [hidden_size]\n",
    "        step_m[\"hidden_norm\"] = torch.norm(final_layer_vec).item()\n",
    "        \n",
    "        metrics_per_step.append(step_m)\n",
    "        \n",
    "        # sample next token (you can switch to top-k or temperature sampling easily)\n",
    "        next_token = torch.multinomial(torch.softmax(logits, dim=-1), num_samples=1)\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "    \n",
    "    gen_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "    return gen_text, metrics_per_step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e94c725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_metrics(metrics_list, top_k: int = 5):\n",
    "    import statistics\n",
    "    entropies     = [m[\"entropy\"] for m in metrics_list]\n",
    "    top1_probs    = [m[\"top1_prob\"] for m in metrics_list]\n",
    "    topk_entropies= [m[f\"top{top_k}_entropy\"] for m in metrics_list]\n",
    "    hidden_norms  = [m[\"hidden_norm\"] for m in metrics_list]\n",
    "    \n",
    "    # flatten all head entropies across layers & steps\n",
    "    head_vals = [h for m in metrics_list for layer in m[\"head_entropies\"] for h in layer]\n",
    "    \n",
    "    return {\n",
    "        \"steps\": len(metrics_list),\n",
    "        \"mean_entropy\": statistics.mean(entropies),\n",
    "        \"stdev_entropy\": statistics.stdev(entropies) if len(entropies)>1 else 0.0,\n",
    "        \"mean_top1_prob\": statistics.mean(top1_probs),\n",
    "        \"stdev_top1_prob\": statistics.stdev(top1_probs) if len(top1_probs)>1 else 0.0,\n",
    "        f\"mean_top{top_k}_entropy\": statistics.mean(topk_entropies),\n",
    "        f\"stdev_top{top_k}_entropy\": statistics.stdev(topk_entropies) if len(topk_entropies)>1 else 0.0,\n",
    "        \"mean_hidden_norm\": statistics.mean(hidden_norms),\n",
    "        \"stdev_hidden_norm\": statistics.stdev(hidden_norms) if len(hidden_norms)>1 else 0.0,\n",
    "        \"mean_head_entropy\": statistics.mean(head_vals),\n",
    "        \"stdev_head_entropy\": statistics.stdev(head_vals) if len(head_vals)>1 else 0.0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2b4fe46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== gpt2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: In a distant future, machines and humans are going to achieve far greater well paid jobs and greater advancement.\n",
      "\n",
      "That might sound pleasant, but it is absolutely not. I am an unemployed\n",
      "Summary: {'steps': 30, 'mean_entropy': 5.455578627189, 'stdev_entropy': 2.4683623503019554, 'mean_top1_prob': 0.32255162329723436, 'stdev_top1_prob': 0.2675351130429084, 'mean_top5_entropy': 1.7119814194117984, 'stdev_top5_entropy': 0.6229998147128664, 'mean_hidden_norm': 260.7959732055664, 'stdev_hidden_norm': 53.4388777045231, 'mean_head_entropy': 2.1144040869251075, 'stdev_head_entropy': 1.1813163152192292}\n",
      " Step 01: entropy=6.04, top1=0.143, top5_entropy=2.26, hidden_norm=250.78\n",
      " Step 02: entropy=9.33, top1=0.031, top5_entropy=2.29, hidden_norm=271.39\n",
      " Step 03: entropy=1.04, top1=0.905, top5_entropy=0.31, hidden_norm=145.79\n",
      " Step 04: entropy=7.15, top1=0.201, top5_entropy=1.72, hidden_norm=278.85\n",
      " Step 05: entropy=7.92, top1=0.108, top5_entropy=2.04, hidden_norm=232.24\n",
      " Step 06: entropy=2.92, top1=0.463, top5_entropy=1.58, hidden_norm=207.89\n",
      " Step 07: entropy=7.98, top1=0.088, top5_entropy=2.12, hidden_norm=251.57\n",
      " Step 08: entropy=1.02, top1=0.912, top5_entropy=0.28, hidden_norm=132.82\n",
      " Step 09: entropy=4.17, top1=0.429, top5_entropy=1.73, hidden_norm=241.01\n",
      " Step 10: entropy=3.83, top1=0.362, top5_entropy=2.04, hidden_norm=235.10\n",
      " Step 11: entropy=9.15, top1=0.037, top5_entropy=2.25, hidden_norm=279.44\n",
      " Step 12: entropy=8.04, top1=0.057, top5_entropy=2.27, hidden_norm=281.21\n",
      " Step 13: entropy=4.12, top1=0.225, top5_entropy=2.08, hidden_norm=209.32\n",
      " Step 14: entropy=6.98, top1=0.147, top5_entropy=2.09, hidden_norm=357.29\n",
      " Step 15: entropy=0.13, top1=0.992, top5_entropy=0.02, hidden_norm=257.00\n",
      " Step 16: entropy=8.28, top1=0.080, top5_entropy=2.18, hidden_norm=307.84\n",
      " Step 17: entropy=5.16, top1=0.386, top5_entropy=1.49, hidden_norm=259.45\n",
      " Step 18: entropy=4.44, top1=0.246, top5_entropy=2.13, hidden_norm=236.84\n",
      " Step 19: entropy=6.51, top1=0.312, top5_entropy=1.30, hidden_norm=255.50\n",
      " Step 20: entropy=3.43, top1=0.423, top5_entropy=1.83, hidden_norm=184.11\n",
      " Step 21: entropy=1.97, top1=0.791, top5_entropy=0.63, hidden_norm=252.19\n",
      " Step 22: entropy=6.76, top1=0.200, top5_entropy=2.00, hidden_norm=330.99\n",
      " Step 23: entropy=4.07, top1=0.407, top5_entropy=1.69, hidden_norm=282.64\n",
      " Step 24: entropy=6.03, top1=0.311, top5_entropy=1.58, hidden_norm=292.93\n",
      " Step 25: entropy=5.94, top1=0.288, top5_entropy=1.76, hidden_norm=261.42\n",
      " Step 26: entropy=3.63, top1=0.527, top5_entropy=1.46, hidden_norm=240.31\n",
      " Step 27: entropy=6.55, top1=0.189, top5_entropy=2.06, hidden_norm=344.90\n",
      " Step 28: entropy=6.35, top1=0.110, top5_entropy=2.19, hidden_norm=359.10\n",
      " Step 29: entropy=7.26, top1=0.168, top5_entropy=1.90, hidden_norm=312.88\n",
      " Step 30: entropy=7.46, top1=0.139, top5_entropy=2.09, hidden_norm=271.07\n",
      "\n",
      "=== TinyLlama/TinyLlama-1.1B-Chat-v1.0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: In a distant future, machines and humans have all become intelligent. Can you summarize what is happening in the story and provide an insight into the alternate future presented in it? \n",
      "Summary: {'steps': 30, 'mean_entropy': 2.592005740243864, 'stdev_entropy': 1.8651399013238552, 'mean_top1_prob': 0.5707355871796608, 'stdev_top1_prob': 0.31219541998329237, 'mean_top5_entropy': 1.2850046824198216, 'stdev_top5_entropy': 0.8164455741263311, 'mean_hidden_norm': 85.30977300008138, 'stdev_hidden_norm': 1.5995957996183812, 'mean_head_entropy': 1.73753543563386, 'stdev_head_entropy': 1.163007158779073}\n",
      " Step 01: entropy=4.06, top1=0.302, top5_entropy=1.99, hidden_norm=85.64\n",
      " Step 02: entropy=5.08, top1=0.157, top5_entropy=2.25, hidden_norm=86.45\n",
      " Step 03: entropy=1.00, top1=0.891, top5_entropy=0.48, hidden_norm=86.51\n",
      " Step 04: entropy=5.19, top1=0.227, top5_entropy=1.90, hidden_norm=87.23\n",
      " Step 05: entropy=0.01, top1=0.999, top5_entropy=0.01, hidden_norm=84.05\n",
      " Step 06: entropy=3.45, top1=0.276, top5_entropy=2.19, hidden_norm=85.29\n",
      " Step 07: entropy=6.31, top1=0.213, top5_entropy=1.99, hidden_norm=86.85\n",
      " Step 08: entropy=1.24, top1=0.884, top5_entropy=0.36, hidden_norm=82.31\n",
      " Step 09: entropy=4.34, top1=0.243, top5_entropy=2.15, hidden_norm=85.64\n",
      " Step 10: entropy=0.01, top1=1.000, top5_entropy=0.01, hidden_norm=80.15\n",
      " Step 11: entropy=0.65, top1=0.950, top5_entropy=0.16, hidden_norm=87.44\n",
      " Step 12: entropy=4.86, top1=0.408, top5_entropy=1.53, hidden_norm=85.88\n",
      " Step 13: entropy=2.87, top1=0.581, top5_entropy=1.33, hidden_norm=85.27\n",
      " Step 14: entropy=0.49, top1=0.941, top5_entropy=0.37, hidden_norm=85.37\n",
      " Step 15: entropy=1.94, top1=0.772, top5_entropy=0.85, hidden_norm=84.39\n",
      " Step 16: entropy=3.45, top1=0.553, top5_entropy=1.39, hidden_norm=82.72\n",
      " Step 17: entropy=2.78, top1=0.442, top5_entropy=1.85, hidden_norm=85.29\n",
      " Step 18: entropy=3.79, top1=0.189, top5_entropy=2.26, hidden_norm=85.27\n",
      " Step 19: entropy=3.04, top1=0.480, top5_entropy=1.79, hidden_norm=86.30\n",
      " Step 20: entropy=3.13, top1=0.246, top5_entropy=2.11, hidden_norm=86.45\n",
      " Step 21: entropy=0.93, top1=0.837, top5_entropy=0.83, hidden_norm=85.24\n",
      " Step 22: entropy=1.40, top1=0.790, top5_entropy=1.01, hidden_norm=85.69\n",
      " Step 23: entropy=5.66, top1=0.168, top5_entropy=2.18, hidden_norm=85.98\n",
      " Step 24: entropy=2.76, top1=0.406, top5_entropy=1.88, hidden_norm=84.96\n",
      " Step 25: entropy=4.60, top1=0.160, top5_entropy=2.28, hidden_norm=85.51\n",
      " Step 26: entropy=1.68, top1=0.462, top5_entropy=1.57, hidden_norm=85.71\n",
      " Step 27: entropy=1.80, top1=0.677, top5_entropy=1.20, hidden_norm=83.78\n",
      " Step 28: entropy=0.18, top1=0.979, top5_entropy=0.15, hidden_norm=83.58\n",
      " Step 29: entropy=1.06, top1=0.887, top5_entropy=0.47, hidden_norm=87.35\n",
      " Step 30: entropy=0.00, top1=1.000, top5_entropy=0.00, hidden_norm=87.01\n"
     ]
    }
   ],
   "source": [
    "for model_name in [\"gpt2\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"]:\n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    prompt = \"In a distant future, machines and humans\"\n",
    "    text, metrics = generate_and_analyze(model, tokenizer, prompt, max_new_tokens=30, top_k=5)\n",
    "    \n",
    "    print(\"Generated:\", text)\n",
    "    summary = summarize_metrics(metrics, top_k=5)\n",
    "    print(\"Summary:\", summary)\n",
    "    # Optional per-step detail:\n",
    "    for i, m in enumerate(metrics, 1):\n",
    "        print(f\" Step {i:02d}: entropy={m['entropy']:.2f}, top1={m['top1_prob']:.3f}, \"\n",
    "                f\"top5_entropy={m['top5_entropy']:.2f}, hidden_norm={m['hidden_norm']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83c4e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
